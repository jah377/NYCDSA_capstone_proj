{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loan Club: Machine Learning Capstone Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from time import strptime  # format data columns\n",
    "import warnings\n",
    "import math\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # ignore warnings throughout notebook\n",
    "pd.set_option(\"display.max_columns\", None)  # show all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "filepath = \"../data/accepted_subsampled_5percent.csv\" #will be personalized\n",
    "df = pd.read_csv(filepath, sep=\",\")\n",
    "\n",
    "df_cleaned = df.copy() #work from second copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features known to investors based on LC website\n",
    "known_vars = ['acc_now_delinq',             # accounts now deliquent\n",
    "              'collections_12_mths_ex_med', # collections excluding medical\n",
    "              'fico_range_high',            # credit score range\n",
    "              'fico_range_low',             # creit score range\n",
    "              'delinq_2yrs',                # delinquencies in last two years\n",
    "              'delinq_amnt',                # delinquency amount\n",
    "              'earliest_cr_line',           # earliest credit line\n",
    "              'home_ownership',             # home ownership\n",
    "              'dti',                        # debt2income ratio\n",
    "              'annual_inc',                 # annual income\n",
    "              'initial_list_status',        # initial listing status\n",
    "              'inq_last_6mths',             # credit inquires in last 6mo\n",
    "              'int_rate',                   # interest rate\n",
    "              'verification_status_joint',  # is this a joint app\n",
    "              'emp_length',                 # length of employment (yr)\n",
    "              'loan_amnt',                  # loan amount\n",
    "              'id',                         # loan id\n",
    "              'purpose',                    # purpose of the loan\n",
    "              'term',                       # loan term (3 or 5yr)\n",
    "              'addr_state',                 # borrower location state\n",
    "              'installment',                # montly payment\n",
    "              'mths_since_last_delinq',     # mo since last delinquency\n",
    "              'mths_since_last_major_derog',# mo since last maj. derogatory\n",
    "              'mths_since_last_record',     # mo since last public record\n",
    "              'open_acc',                   # open credit line\n",
    "              'pub_rec',                    # public records on file\n",
    "              'revol_util',                 # revolving balance utilization (%)\n",
    "              'revol_bal',                  # revolving credit balance ($)\n",
    "              'tot_coll_amt',               # total collection amount ever\n",
    "              'total_acc',                  # total credit lines\n",
    "              'tot_cur_bal',                # total current balance\n",
    "              'verification_status',        # verified income (Y/N I think)\n",
    "              'grade'                       # loan grade\n",
    "             ]\n",
    "\n",
    "# Sanity check, print variable if not found within original dataframe\n",
    "# [ print(var) for var in known_vars if (var not in df.columns)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute Missing Data of Known Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "verification_status_joint      0.949313\n",
       "mths_since_last_record         0.838543\n",
       "mths_since_last_major_derog    0.744395\n",
       "mths_since_last_delinq         0.513786\n",
       "emp_length                     0.063868\n",
       "tot_cur_bal                    0.033430\n",
       "tot_coll_amt                   0.033430\n",
       "revol_util                     0.001006\n",
       "dti                            0.000829\n",
       "collections_12_mths_ex_med     0.000476\n",
       "open_acc                       0.000185\n",
       "pub_rec                        0.000185\n",
       "total_acc                      0.000185\n",
       "inq_last_6mths                 0.000185\n",
       "earliest_cr_line               0.000185\n",
       "delinq_amnt                    0.000185\n",
       "delinq_2yrs                    0.000185\n",
       "acc_now_delinq                 0.000185\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Assess missingness of known variables\n",
    "missingness = df_cleaned[known_vars].isnull().mean().T\n",
    "missingness = missingness.loc[missingness>0].sort_values(ascending=False)\n",
    "missingness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop features missing > 50% \n",
    "feat_wManyMissing = missingness.index[np.where(missingness > .5)].to_list()\n",
    "df_cleaned[known_vars].drop(df_cleaned[feat_wManyMissing], axis=1, inplace=True)\n",
    "[known_vars.remove(var) for var in feat_wManyMissing] #remove features from known_var list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace NA's of numeric 'known_var' features with mean value\n",
    "numeric_var = ['tot_cur_bal',\n",
    "               'tot_coll_amt',\n",
    "               'revol_util',\n",
    "               'collections_12_mths_ex_med',\n",
    "               'open_acc',\n",
    "               'pub_rec',\n",
    "               'total_acc',\n",
    "               'inq_last_6mths',\n",
    "               'delinq_amnt',\n",
    "               'delinq_2yrs',\n",
    "               'dti' ]\n",
    "\n",
    "# List comprehension through numerica variables\n",
    "[df_cleaned[var].fillna(df[var].mean(), inplace=True) for var in numeric_var]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Helper function to replace missing character strings with randomly selected value\n",
    "def fillna_random(var):\n",
    "    #find index of missing values\n",
    "    miss_idx = df_cleaned.loc[ df_cleaned[var].isnull()].index.tolist()\n",
    "    \n",
    "    #find new values to replace NaN values\n",
    "    new_val = df_cleaned[var].loc[~df_cleaned.index.isin(miss_idx)].sample(len(miss_idx)).values.tolist()\n",
    "\n",
    "    #replace values\n",
    "    df_cleaned[var][miss_idx] = new_val\n",
    "\n",
    "# ==================================================\n",
    "# Replace NA's of character 'known_var' features with random\n",
    "non_numeric_var = ['emp_length', 'earliest_cr_line', 'acc_now_delinq', 'delinq_2yrs'] #list of non-numeric variables\n",
    "[fillna_random(var) for var in non_numeric_var]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "acc_now_delinq                0\n",
       "collections_12_mths_ex_med    0\n",
       "fico_range_high               0\n",
       "fico_range_low                0\n",
       "delinq_2yrs                   0\n",
       "delinq_amnt                   0\n",
       "earliest_cr_line              0\n",
       "home_ownership                0\n",
       "dti                           0\n",
       "annual_inc                    0\n",
       "initial_list_status           0\n",
       "inq_last_6mths                0\n",
       "int_rate                      0\n",
       "emp_length                    0\n",
       "loan_amnt                     0\n",
       "id                            0\n",
       "purpose                       0\n",
       "term                          0\n",
       "addr_state                    0\n",
       "installment                   0\n",
       "open_acc                      0\n",
       "pub_rec                       0\n",
       "revol_util                    0\n",
       "revol_bal                     0\n",
       "tot_coll_amt                  0\n",
       "total_acc                     0\n",
       "tot_cur_bal                   0\n",
       "verification_status           0\n",
       "grade                         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sanity check that no more missing values\n",
    "df_cleaned[known_vars].isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplify loan status (non-FullyPaid or ChargedOff loans will be converted to NAN)\n",
    "df_cleaned['loan_status'] = df_cleaned['loan_status'].map({'Fully Paid':'Fully Paid',\n",
    "                                                           'Charged Off':'Charged Off',\n",
    "                                                           'Does not meet the credit policy. Status:Fully Paid': 'Fully Paid',\n",
    "                                                           'Does not meet the credit policy. Status:Charged Off': 'Charged Off'})\n",
    "\n",
    "# Remove non-completed loans\n",
    "df_cleaned.drop(df_cleaned.loc[df_cleaned['loan_status'].isnull()].index.tolist(), axis=0, inplace=True)\n",
    "\n",
    "# Simplify home ownership\n",
    "df_cleaned['home_ownership'] = df_cleaned['home_ownership'].map({'MORTGAGE':'mortgage',\n",
    "                                                                 'OWN':'own',\n",
    "                                                                 'RENT':'rent'})\n",
    "\n",
    "# Remove 25 observations without houses\n",
    "df_cleaned.drop(df_cleaned.loc[df_cleaned['home_ownership'].isnull()].index.tolist(), axis=0, inplace=True)\n",
    "\n",
    "\n",
    "# Reformat date features and calculate features related to prepayment  \n",
    "df_cleaned['term_year'] = np.where(df_cleaned['term']==' 36 months', 3,5)\n",
    "df_cleaned['earliest_cr_line'] =  pd.to_datetime(df_cleaned['earliest_cr_line'])\n",
    "df_cleaned['issue_date'] =  pd.to_datetime(df_cleaned['issue_d'])\n",
    "df_cleaned['last_pymnt_date'] = pd.to_datetime(df_cleaned['last_pymnt_d'])\n",
    "df_cleaned['exp_last_pymnt_date'] = pd.to_datetime(df_cleaned['issue_d'].str[0:3]\n",
    "                                                   +'-'\n",
    "                                                   + (df_cleaned['issue_d'].str[-4:].astype('int')\n",
    "                                                   + df_cleaned['term_year']).astype('str'))\n",
    "\n",
    "# Calculate credit history ( in months )\n",
    "date_ofloan = df_cleaned['issue_date'].dt.to_period('M').astype(int)\n",
    "date_credline = df_cleaned['earliest_cr_line'].dt.to_period('M').astype(int)\n",
    "df_cleaned['credit_hist_mths'] = date_ofloan - date_credline\n",
    "df_cleaned['credit_hist_mths'] = np.where(df_cleaned['credit_hist_mths'] < 0, 0, df_cleaned['credit_hist_mths'])\n",
    "\n",
    "# Log-transform skewed continuous features\n",
    "df_cleaned['delinq_amnt_log'] = df_cleaned['delinq_amnt'].add(1).apply(np.log)\n",
    "df_cleaned['annual_inc_log'] = df_cleaned['annual_inc'].add(1).apply(np.log)\n",
    "df_cleaned['dti_log'] = df_cleaned['dti'].add(1).apply(np.log)\n",
    "df_cleaned['funded_amnt_log'] = df_cleaned['funded_amnt'].add(1).apply(np.log)\n",
    "df_cleaned['tot_coll_amt_log'] = df_cleaned['tot_coll_amt'].add(1).apply(np.log)\n",
    "df_cleaned['tot_cur_bal_log'] = df_cleaned['tot_cur_bal'].add(1).apply(np.log)\n",
    "df_cleaned['total_acc_log'] = df_cleaned['total_acc'].add(1).apply(np.log)\n",
    "df_cleaned['revol_bal_log'] = df_cleaned['revol_bal'].add(1).apply(np.log)\n",
    "df_cleaned['installment_log'] = df_cleaned['installment'].add(1).apply(np.log)\n",
    "df_cleaned['open_acc_log'] = df_cleaned['open_acc'].add(1).apply(np.log)\n",
    "\n",
    "# Simplify loan purpose - debt consolidation, credit card, and other\n",
    "df_cleaned['purpose'] = df_cleaned['purpose'].map({'debt_consolidation':'debt_consolidation',\n",
    "                                                   'credit_card':'credit_card'})\n",
    "df_cleaned['purpose'].fillna('other',inplace=True)\n",
    "\n",
    "# Convert loan grade to ordinal feature\n",
    "df_cleaned['grade'] = df_cleaned['grade'].map({'A':1,\n",
    "                                               'B':2,\n",
    "                                               'C':3,\n",
    "                                               'D':4,\n",
    "                                               'E':5,\n",
    "                                               'F':6,\n",
    "                                               'G':7})\n",
    "\n",
    "# Simplify employment length to four categories\n",
    "df_cleaned['emp_length'] = df_cleaned['emp_length'].map({'< 1 year':0.5,\n",
    "                                                         '1 year':1,\n",
    "                                                         '2 years':2,\n",
    "                                                         '3 years':3,\n",
    "                                                         '4 years':4,\n",
    "                                                         '5 years':5,\n",
    "                                                         '6 years':6,\n",
    "                                                         '7 years':7,\n",
    "                                                         '8 years':8,\n",
    "                                                         '9 years':9,\n",
    "                                                         '10+ years':10})\n",
    "\n",
    "# Create new binary features\n",
    "df_cleaned['has_pub_rec'] = np.where(df_cleaned['pub_rec']>0,1,0) #0-=no public record\n",
    "df_cleaned['has_paid_early'] = np.where((df_cleaned.loan_status=='Fully Paid')&(df_cleaned.last_pymnt_date < df_cleaned.exp_last_pymnt_date), 1, 0)\n",
    "df_cleaned['has_36mo_loan'] = np.where(df_cleaned['term'].str.contains('36'),1,0) #0=60mo loan\n",
    "df_cleaned['has_delinq_now'] = np.where(df_cleaned['acc_now_delinq']>0, 1, 0)\n",
    "df_cleaned['has_delinq_past2yrs'] = np.where(df_cleaned['delinq_2yrs']>0, 1, 0) #0=no delinq within 2yrs\n",
    "df_cleaned['has_whole_liststatus'] = np.where(df_cleaned['initial_list_status']=='w', 1, 0) #0=f\n",
    "df_cleaned['has_fullypaid'] = np.where(df_cleaned['loan_status']=='Fully Paid', 1, 0) #0=charged off\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of loans above threadhold: 0.838\n"
     ]
    }
   ],
   "source": [
    "# Create response variable based on \n",
    "threshold = -20 #goal % return\n",
    "df_cleaned[\"roi_perc\"] = df_cleaned[\"total_pymnt\"].div(df_cleaned[\"funded_amnt\"]).sub(1).mul(100)\n",
    "df_cleaned['roi_response'] = np.where(df_cleaned['roi_perc'] > threshold, 1, 0)\n",
    "\n",
    "print('Fraction of loans above threadhold: %.3f' % df_cleaned['roi_response'].mean())\n",
    "\n",
    "# ==================================================================\n",
    "# Majority class [1] = ROI >-20% (IDEAL)\n",
    "# Minority class [0] = ROI <-20% (BAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate list of predictor variables to be used for ML\n",
    "predictor_vars = ['annual_inc_log',\n",
    "                  'credit_hist_mths',\n",
    "                  'delinq_amnt_log',\n",
    "                  'dti_log',\n",
    "                  'emp_length',\n",
    "                  'fico_range_high',\n",
    "                  'funded_amnt_log',\n",
    "                  'grade',\n",
    "                  'has_36mo_loan',\n",
    "                  'has_delinq_now',\n",
    "                  'has_delinq_past2yrs',\n",
    "                  'has_pub_rec',\n",
    "                  'has_whole_liststatus',\n",
    "                  'home_ownership',\n",
    "                  'inq_last_6mths',\n",
    "                  'installment_log',\n",
    "                  'int_rate',\n",
    "                  'open_acc_log',\n",
    "                  'purpose',\n",
    "                  'revol_bal_log',\n",
    "                  'revol_util',\n",
    "                  'tot_coll_amt_log',\n",
    "                  'tot_cur_bal_log',\n",
    "                  'total_acc_log',\n",
    "                  'verification_status']\n",
    "\n",
    "response_var = 'roi_response'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "- Goal is to build classifier that predicts if loan results in desirable outcome\n",
    "- Should also report feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummify categorical features\n",
    "home_ownership_dummy = pd.get_dummies(df_cleaned['home_ownership'],\n",
    "                                      prefix=\"home_ownership\").drop('home_ownership_mortgage',axis=1)\n",
    "\n",
    "purpose_dummy = pd.get_dummies(df_cleaned['purpose'],\n",
    "                               prefix=\"purpose\").drop('purpose_debt_consolidation',axis=1)\n",
    "\n",
    "verification_status_dummy = pd.get_dummies(df_cleaned['verification_status'],\n",
    "                               prefix=\"verification_status\").drop('verification_status_Source Verified',axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of numeric features in the final dataframe\n",
    "numeric_vars = ['annual_inc_log',\n",
    "                'credit_hist_mths',\n",
    "                'delinq_amnt_log',\n",
    "                'dti_log',\n",
    "                'emp_length',\n",
    "                'fico_range_high',\n",
    "                'funded_amnt_log',\n",
    "                'grade',\n",
    "                'has_36mo_loan',\n",
    "                'has_delinq_now',\n",
    "                'has_delinq_past2yrs',\n",
    "                'has_pub_rec',\n",
    "                'has_whole_liststatus',\n",
    "                'inq_last_6mths',\n",
    "                'installment_log',\n",
    "                'int_rate',\n",
    "                'open_acc_log',\n",
    "                'revol_bal_log',\n",
    "                'revol_util',\n",
    "                'tot_coll_amt_log',\n",
    "                'tot_cur_bal_log',\n",
    "                'total_acc_log']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final dataframe for model consumption\n",
    "df_feature_final = pd.concat([df_cleaned[numeric_vars],\n",
    "                      home_ownership_dummy,\n",
    "                      purpose_dummy,\n",
    "                      verification_status_dummy],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree-Based Model\n",
    "- Goal is to build classifier that predicts if loan results in desirable outcome\n",
    "- Should also report feature importance\n",
    "\n",
    "- Initially explored Random Forest algorithm but switched to XGboost due to time constraint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, precision_score, auc, recall_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive XGBoost Model - Imbalanced Data (83.8% ROI>-20%)\n",
    "\n",
    "*Goal is to minimize false positives* - Maximixe precision\n",
    "- Majority class [1] = ROI >-20% (IDEAL)\n",
    "- Minority class [0] = ROI <-20% (BAD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create test-train split of data\n",
    "x = df_feature_final\n",
    "y = df_cleaned[response_var] #labels\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auc_PR': 0.8409,\n",
       " 'accuracy': 0.8384,\n",
       " 'recall': 0.9957,\n",
       " 'precision': 0.8409,\n",
       " 'false_discovery_rate': 0.1591,\n",
       " 'false_pos_rate': 0.9799,\n",
       " 'error_rate': 0.1616,\n",
       " 'n_true_pos': 11325,\n",
       " 'n_false_pos': 2143}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create random forest object\n",
    "naive_model = xgb.XGBClassifier(objective = 'binary:logistic', \n",
    "                                seed = 0)\n",
    "\n",
    "#Fit model with data\n",
    "naive_model.fit(x_train, y_train)\n",
    "\n",
    "#Analyze\n",
    "model = naive_model\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, model.predict(x_test)).ravel()\n",
    "results = {'auc_PR': round(average_precision_score(y_test, model.predict(x_test)),4),\n",
    "           'accuracy': round((tp + tn) / (tp + tn + fn + fp),4),\n",
    "           'recall': round(tp / (tp + fn),4),\n",
    "           'precision': round(tp/(tp+fp),4),\n",
    "           'false_discovery_rate': round(fp/(fp+tp),4),\n",
    "           'false_pos_rate': round(fp / (tn + fp),4),\n",
    "           'error_rate': round((fp + fn) / (tp + tn + fn + fp),4),\n",
    "           'n_true_pos':tp,\n",
    "           'n_false_pos':fp}\n",
    "results\n",
    "\n",
    "# # ==================== Print Results ====================\n",
    "# model = naive_model\n",
    "# y_pred = model.predict(x_test)\n",
    "# auc_PR = average_precision_score(y_test, y_pred)\n",
    "# precision, recall, _ = precision_recall_curve(y_test, y_pred)\n",
    "# plt.plot(recall, precision)\n",
    "# plt.xlabel('Recall (True Positive) ')\n",
    "# plt.ylabel('Precision (Proportion Correct)')\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.title('Binary Precision-Recall curve: '\n",
    "#                    'auc_PR={0:0.2f}'.format(auc_PR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive XGBoost Model - using scale_pos_weight parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weight': 0.1929,\n",
       " 'auc_PR': 0.8827,\n",
       " 'accuracy': 0.6406,\n",
       " 'recall': 0.636,\n",
       " 'precision': 0.9079,\n",
       " 'false_discovery_rate': 0.0921,\n",
       " 'false_pos_rate': 0.3356,\n",
       " 'error_rate': 0.3594,\n",
       " 'n_true_pos': 7234,\n",
       " 'n_false_pos': 734}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create random forest object\n",
    "#scale_pos_weight = sum(negative) / sum(positive)\n",
    "naive_spw_model = xgb.XGBClassifier(objective = 'binary:logistic',\n",
    "                                    scale_pos_weight=(len(y)-y.sum())/y.sum(), #0.1929\n",
    "                                    seed = 0)\n",
    "\n",
    "#Fit model with data\n",
    "naive_spw_model.fit(x_train, y_train)\n",
    "\n",
    "#Analyze\n",
    "model = naive_spw_model\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, model.predict(x_test)).ravel()\n",
    "naive_spw_results = {'weight': round((len(y)-y.sum())/y.sum(),4),\n",
    "                     'auc_PR': round(average_precision_score(y_test, model.predict(x_test)),4),\n",
    "                     'accuracy': round((tp + tn) / (tp + tn + fn + fp),4),\n",
    "                     'recall': round(tp / (tp + fn),4),\n",
    "                     'precision': round(tp/(tp+fp),4),\n",
    "                     'false_discovery_rate': round(fp/(fp+tp),4),\n",
    "                     'false_pos_rate': round(fp / (tn + fp),4),\n",
    "                     'error_rate': round((fp + fn) / (tp + tn + fn + fp),4),\n",
    "                     'n_true_pos':tp,\n",
    "                     'n_false_pos':fp}\n",
    "naive_spw_results\n",
    "\n",
    "# # ==================== Print Results ====================\n",
    "# model = naive_model\n",
    "# y_pred = model.predict(x_test)\n",
    "# auc_PR = average_precision_score(y_test, y_pred)\n",
    "# precision, recall, _ = precision_recall_curve(y_test, y_pred)\n",
    "# plt.plot(recall, precision)\n",
    "# plt.xlabel('Recall (True Positive) ')\n",
    "# plt.ylabel('Precision (Proportion Correct)')\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.title('Binary Precision-Recall curve: '\n",
    "#                    'auc_PR={0:0.2f}'.format(auc_PR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune scale_pos_weight parameter based on area under the Precision-Recall curve (auc_PR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 47s, sys: 629 ms, total: 2min 48s\n",
      "Wall time: 2min 48s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight</th>\n",
       "      <th>auc_PR</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>false_discovery_rate</th>\n",
       "      <th>false_pos_rate</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>n_true_pos</th>\n",
       "      <th>n_false_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.8834</td>\n",
       "      <td>0.6381</td>\n",
       "      <td>0.6314</td>\n",
       "      <td>0.9094</td>\n",
       "      <td>0.0906</td>\n",
       "      <td>0.3269</td>\n",
       "      <td>0.3619</td>\n",
       "      <td>7181.0</td>\n",
       "      <td>715.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.8830</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.5761</td>\n",
       "      <td>0.9156</td>\n",
       "      <td>0.0844</td>\n",
       "      <td>0.2762</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>6553.0</td>\n",
       "      <td>604.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.8830</td>\n",
       "      <td>0.5783</td>\n",
       "      <td>0.5446</td>\n",
       "      <td>0.9199</td>\n",
       "      <td>0.0801</td>\n",
       "      <td>0.2465</td>\n",
       "      <td>0.4217</td>\n",
       "      <td>6194.0</td>\n",
       "      <td>539.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.8829</td>\n",
       "      <td>0.6178</td>\n",
       "      <td>0.6023</td>\n",
       "      <td>0.9121</td>\n",
       "      <td>0.0879</td>\n",
       "      <td>0.3018</td>\n",
       "      <td>0.3822</td>\n",
       "      <td>6851.0</td>\n",
       "      <td>660.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.8824</td>\n",
       "      <td>0.5576</td>\n",
       "      <td>0.5152</td>\n",
       "      <td>0.9236</td>\n",
       "      <td>0.0764</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.4424</td>\n",
       "      <td>5860.0</td>\n",
       "      <td>485.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.8824</td>\n",
       "      <td>0.5357</td>\n",
       "      <td>0.4832</td>\n",
       "      <td>0.9292</td>\n",
       "      <td>0.0708</td>\n",
       "      <td>0.1916</td>\n",
       "      <td>0.4643</td>\n",
       "      <td>5496.0</td>\n",
       "      <td>419.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.8822</td>\n",
       "      <td>0.6522</td>\n",
       "      <td>0.6538</td>\n",
       "      <td>0.9053</td>\n",
       "      <td>0.0947</td>\n",
       "      <td>0.3557</td>\n",
       "      <td>0.3478</td>\n",
       "      <td>7436.0</td>\n",
       "      <td>778.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.8812</td>\n",
       "      <td>0.5117</td>\n",
       "      <td>0.4501</td>\n",
       "      <td>0.9331</td>\n",
       "      <td>0.0669</td>\n",
       "      <td>0.1678</td>\n",
       "      <td>0.4883</td>\n",
       "      <td>5119.0</td>\n",
       "      <td>367.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.8802</td>\n",
       "      <td>0.4865</td>\n",
       "      <td>0.4148</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.0613</td>\n",
       "      <td>0.1408</td>\n",
       "      <td>0.5135</td>\n",
       "      <td>4718.0</td>\n",
       "      <td>308.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.8783</td>\n",
       "      <td>0.4579</td>\n",
       "      <td>0.3759</td>\n",
       "      <td>0.9439</td>\n",
       "      <td>0.0561</td>\n",
       "      <td>0.1161</td>\n",
       "      <td>0.5421</td>\n",
       "      <td>4276.0</td>\n",
       "      <td>254.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.8757</td>\n",
       "      <td>0.4299</td>\n",
       "      <td>0.3389</td>\n",
       "      <td>0.9479</td>\n",
       "      <td>0.0521</td>\n",
       "      <td>0.0969</td>\n",
       "      <td>0.5701</td>\n",
       "      <td>3855.0</td>\n",
       "      <td>212.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.8717</td>\n",
       "      <td>0.3971</td>\n",
       "      <td>0.2969</td>\n",
       "      <td>0.9497</td>\n",
       "      <td>0.0503</td>\n",
       "      <td>0.0818</td>\n",
       "      <td>0.6029</td>\n",
       "      <td>3377.0</td>\n",
       "      <td>179.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.8674</td>\n",
       "      <td>0.3635</td>\n",
       "      <td>0.2540</td>\n",
       "      <td>0.9516</td>\n",
       "      <td>0.0484</td>\n",
       "      <td>0.0672</td>\n",
       "      <td>0.6365</td>\n",
       "      <td>2889.0</td>\n",
       "      <td>147.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.8643</td>\n",
       "      <td>0.3354</td>\n",
       "      <td>0.2175</td>\n",
       "      <td>0.9563</td>\n",
       "      <td>0.0437</td>\n",
       "      <td>0.0517</td>\n",
       "      <td>0.6646</td>\n",
       "      <td>2474.0</td>\n",
       "      <td>113.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.8603</td>\n",
       "      <td>0.3041</td>\n",
       "      <td>0.1777</td>\n",
       "      <td>0.9601</td>\n",
       "      <td>0.0399</td>\n",
       "      <td>0.0384</td>\n",
       "      <td>0.6959</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.8566</td>\n",
       "      <td>0.2753</td>\n",
       "      <td>0.1410</td>\n",
       "      <td>0.9657</td>\n",
       "      <td>0.0343</td>\n",
       "      <td>0.0261</td>\n",
       "      <td>0.7247</td>\n",
       "      <td>1604.0</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.8527</td>\n",
       "      <td>0.2492</td>\n",
       "      <td>0.1085</td>\n",
       "      <td>0.9671</td>\n",
       "      <td>0.0329</td>\n",
       "      <td>0.0192</td>\n",
       "      <td>0.7508</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.8477</td>\n",
       "      <td>0.2169</td>\n",
       "      <td>0.0685</td>\n",
       "      <td>0.9701</td>\n",
       "      <td>0.0299</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.7831</td>\n",
       "      <td>779.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.8432</td>\n",
       "      <td>0.1869</td>\n",
       "      <td>0.0310</td>\n",
       "      <td>0.9833</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.8131</td>\n",
       "      <td>353.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.8396</td>\n",
       "      <td>0.1658</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8342</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    weight  auc_PR  accuracy  recall  precision  false_discovery_rate  \\\n",
       "18    0.19  0.8834    0.6381  0.6314     0.9094                0.0906   \n",
       "16    0.17  0.8830    0.6000  0.5761     0.9156                0.0844   \n",
       "15    0.16  0.8830    0.5783  0.5446     0.9199                0.0801   \n",
       "17    0.18  0.8829    0.6178  0.6023     0.9121                0.0879   \n",
       "14    0.15  0.8824    0.5576  0.5152     0.9236                0.0764   \n",
       "13    0.14  0.8824    0.5357  0.4832     0.9292                0.0708   \n",
       "19    0.20  0.8822    0.6522  0.6538     0.9053                0.0947   \n",
       "12    0.13  0.8812    0.5117  0.4501     0.9331                0.0669   \n",
       "11    0.12  0.8802    0.4865  0.4148     0.9387                0.0613   \n",
       "10    0.11  0.8783    0.4579  0.3759     0.9439                0.0561   \n",
       "9     0.10  0.8757    0.4299  0.3389     0.9479                0.0521   \n",
       "8     0.09  0.8717    0.3971  0.2969     0.9497                0.0503   \n",
       "7     0.08  0.8674    0.3635  0.2540     0.9516                0.0484   \n",
       "6     0.07  0.8643    0.3354  0.2175     0.9563                0.0437   \n",
       "5     0.06  0.8603    0.3041  0.1777     0.9601                0.0399   \n",
       "4     0.05  0.8566    0.2753  0.1410     0.9657                0.0343   \n",
       "3     0.04  0.8527    0.2492  0.1085     0.9671                0.0329   \n",
       "2     0.03  0.8477    0.2169  0.0685     0.9701                0.0299   \n",
       "1     0.02  0.8432    0.1869  0.0310     0.9833                0.0167   \n",
       "0     0.01  0.8396    0.1658  0.0054     1.0000                0.0000   \n",
       "\n",
       "    false_pos_rate  error_rate  n_true_pos  n_false_pos  \n",
       "18          0.3269      0.3619      7181.0        715.0  \n",
       "16          0.2762      0.4000      6553.0        604.0  \n",
       "15          0.2465      0.4217      6194.0        539.0  \n",
       "17          0.3018      0.3822      6851.0        660.0  \n",
       "14          0.2218      0.4424      5860.0        485.0  \n",
       "13          0.1916      0.4643      5496.0        419.0  \n",
       "19          0.3557      0.3478      7436.0        778.0  \n",
       "12          0.1678      0.4883      5119.0        367.0  \n",
       "11          0.1408      0.5135      4718.0        308.0  \n",
       "10          0.1161      0.5421      4276.0        254.0  \n",
       "9           0.0969      0.5701      3855.0        212.0  \n",
       "8           0.0818      0.6029      3377.0        179.0  \n",
       "7           0.0672      0.6365      2889.0        147.0  \n",
       "6           0.0517      0.6646      2474.0        113.0  \n",
       "5           0.0384      0.6959      2021.0         84.0  \n",
       "4           0.0261      0.7247      1604.0         57.0  \n",
       "3           0.0192      0.7508      1234.0         42.0  \n",
       "2           0.0110      0.7831       779.0         24.0  \n",
       "1           0.0027      0.8131       353.0          6.0  \n",
       "0           0.0000      0.8342        61.0          0.0  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "store = pd.DataFrame([], columns=['weight',\n",
    "                                  'auc_PR',\n",
    "                                  'accuracy',\n",
    "                                  'recall',\n",
    "                                  'precision',\n",
    "                                  'false_discovery_rate',\n",
    "                                  'false_pos_rate',\n",
    "                                  'error_rate',\n",
    "                                  'n_true_pos',\n",
    "                                  'n_false_pos'])\n",
    "\n",
    "weights = [i/100 for i in range(1,21)]\n",
    "for weight in weights:\n",
    "    #Create random forest object\n",
    "    #scale_pos_weight = sum(negative) / sum(positive)\n",
    "    spw_tuning_model = xgb.XGBClassifier(objective = 'binary:logistic',\n",
    "                                         scale_pos_weight = weight,\n",
    "                                         eval_matrix = 'aucpr',\n",
    "                                         seed = 0) #PR AUC - goal to minimze False positives \n",
    "\n",
    "    #Fit model with data\n",
    "    spw_tuning_model.fit(x_train, y_train)\n",
    "    \n",
    "    # ==================== Analyze ====================\n",
    "    model = spw_tuning_model\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, model.predict(x_test)).ravel()\n",
    "    store = store.append({'weight':weight,\n",
    "                          'auc_PR': round(average_precision_score(y_test, model.predict(x_test)),4),\n",
    "                          'accuracy': round((tp + tn) / (tp + tn + fn + fp),4),\n",
    "                          'recall': round(tp / (tp + fn),4),\n",
    "                          'precision': round(tp/(tp+fp),4),\n",
    "                          'false_discovery_rate': round(fp/(fp+tp),4),\n",
    "                          'false_pos_rate': round(fp / (tn + fp),4),\n",
    "                          'error_rate': round((fp + fn) / (tp + tn + fn + fp),4),\n",
    "                          'n_true_pos':tp,\n",
    "                          'n_false_pos':fp}, ignore_index=True)\n",
    "\n",
    "store = store.sort_values(by='auc_PR',ascending=False)\n",
    "store\n",
    "                     \n",
    "# # ==================== P-R Curve Plot ====================\n",
    "# model = xgb.XGBClassifier(objective = 'binary:logistic',\n",
    "#                           scale_pos_weight=store.weight.iloc[0],\n",
    "#                           eval_matrix = 'aucpr', \n",
    "#                           seed = 0)\n",
    "# model.fit(x_train, y_train)\n",
    "# y_pred = model.predict(x_test)\n",
    "# auc_PR = average_precision_score(y_test, y_pred)\n",
    "# precision, recall, _ = precision_recall_curve(y_test, y_pred)\n",
    "# plt.plot(recall, precision)\n",
    "# plt.xlabel('Recall (True Positive) ')\n",
    "# plt.ylabel('Precision (Proportion Correct)')\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.title('Binary Precision-Recall curve: '\n",
    "#                    'auc_PR={0:0.2f}'.format(auc_PR))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tuning Hyperparameters**\n",
    "- max_depth = depth of trees (3-10)\n",
    "- min_child_weight = similar to min_child_leaf for RandomForest\n",
    "- gamma = min. loss reduction required for split\n",
    "- subsample = frac of observations randomly sampled for tree (0.5-1)\n",
    "- colsample_bytree = number of features sampled for each tree\n",
    "- learning rate = typically between 0.01-0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc, make_scorer, recall_score, accuracy_score, precision_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_wrapper(refit_score='precision_score'):\n",
    "    \"\"\"\n",
    "    fits a GridSearchCV classifier using refit_score for optimization\n",
    "    prints classifier performance metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    grid_search = GridSearchCV(model,\n",
    "                                     param_grid,\n",
    "                                     scoring=scorers,\n",
    "                                     refit=refit_score,\n",
    "                                     cv=skf,\n",
    "#                                      random_state=42,\n",
    "#                                      n_iter=100,\n",
    "                                     verbose=1,\n",
    "                                     return_train_score=True,\n",
    "                                     n_jobs = -1)\n",
    "    \n",
    "    grid_search.fit(x_train, y_train)\n",
    "\n",
    "    # make the predictions\n",
    "    y_pred = grid_search.predict(x_test)\n",
    "\n",
    "    print('Best params for {}'.format(refit_score))\n",
    "    print(grid_search.best_params_)\n",
    "\n",
    "    # confusion matrix on the test data.\n",
    "    print('\\nConfusion matrix of XGBoost optimized for {} on the test data:'.format(refit_score))\n",
    "    print(pd.DataFrame(confusion_matrix(y_test, y_pred),\n",
    "                 columns=['pred_neg', 'pred_pos'], index=['neg', 'pos']))\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    precision = round(tp/(tp+fp),4) * 100\n",
    "    print(\"\\nPrecision: {0}% of loans identified as a good loan (ROI>-20%) were correct\\n\".format(precision))\n",
    "\n",
    "    false_discovery_rate = round(fp/(fp+tp),4) * 100\n",
    "    print(\"False discovery rate: {0}% of loans identified as good [ROI>-20%] were actually bad loans [ROI<-20%]\\n\\n\".format(false_discovery_rate))\n",
    "\n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 900 candidates, totalling 4500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   48.9s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed: 10.9min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 40.8min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed: 104.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed: 151.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed: 237.7min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed: 345.9min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed: 476.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed: 638.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4500 out of 4500 | elapsed: 769.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for precision_score\n",
      "{'colsample_bytree': 0.9, 'max_depth': 1, 'n_estimators': 500, 'subsample': 0.5}\n",
      "\n",
      "Confusion matrix of XGBoost optimized for precision_score on the test data:\n",
      "     pred_neg  pred_pos\n",
      "neg      1479       708\n",
      "pos      4228      7146\n",
      "\n",
      "Precision: 90.99000000000001% of loans identified as a good loan (ROI>-20%) were correct\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'false_discover_rate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-d648f7b6ef68>\u001b[0m in \u001b[0;36mgrid_search_wrapper\u001b[0;34m(refit_score)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mfalse_discovery_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"False discovery rate: {0}% of loans identified as good [ROI>-20%] were actually bad loans [ROI<-20%]\\n\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfalse_discover_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'false_discover_rate' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Based on link: https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(objective = 'binary:logistic',\n",
    "                              early_stopping_rounds = 5,\n",
    "                              scale_pos_weight = (len(y)-y.sum())/y.sum())\n",
    "\n",
    "\n",
    "c\n",
    "\n",
    "scorers = {\n",
    "    'precision_score': make_scorer(precision_score),\n",
    "}\n",
    "\n",
    "grid_search_model = grid_search_wrapper(refit_score='precision_score')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pred_neg  pred_pos\n",
      "neg      1453       734\n",
      "pos      4140      7234\n",
      "\n",
      "Accuracy: 64.06% of loans were correctly identified\n",
      "\n",
      "Specificity: 66.44% of loans with ROI<-20% were identified as a bad loan loan\n",
      "\n",
      "Recall/Sensitivity: 63.6% of loans with ROI>-20% were identified as a good loan\n",
      "\n",
      "Precision: 90.79% of loans identified as a good loan (ROI>-20%) were correct\n",
      "\n",
      "False discovery rate: 9.21% of loans identified as good [ROI>-20%] were actually bad loans [ROI<-20%]\n",
      "\n",
      "False Positive Rate: 33.56% of bad loans [ROI<-20%] were identified as a good loan [ROI>-20%]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Confusion Matrix\n",
    "y_pred = naive_spw_model.predict(x_test)\n",
    "print(pd.DataFrame(confusion_matrix(y_test, y_pred),\n",
    "             columns=['pred_neg', 'pred_pos'], index=['neg', 'pos']))\n",
    "\n",
    "#CM Calculations\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "accuracy = round( 100*(tp + tn) / (tp + tn + fn + fp) ,2)\n",
    "print(\"\\nAccuracy: {0}% of loans were correctly identified\\n\".format(accuracy))\n",
    "\n",
    "specificity = round(tn / (tn + fp),4) * 100 #true negative rate\n",
    "print(\"Specificity: {0}% of loans with ROI<-20% were identified as a bad loan loan\\n\".format(specificity))\n",
    "\n",
    "recall = round(tp / (tp + fn), 4) * 100 #true positive rate\n",
    "print(\"Recall/Sensitivity: {0}% of loans with ROI>-20% were identified as a good loan\\n\".format(recall))\n",
    "\n",
    "precision = round(tp/(tp+fp),4) * 100\n",
    "print(\"Precision: {0}% of loans identified as a good loan (ROI>-20%) were correct\\n\".format(precision))\n",
    "\n",
    "false_discovery_rate = round(fp/(fp+tp),4) * 100\n",
    "print(\"False discovery rate: {0}% of loans identified as good [ROI>-20%] were actually bad loans [ROI<-20%]\\n\".format(false_discovery_rate))\n",
    "\n",
    "false_pos_rate = round(fp / (tn + fp),4) * 100\n",
    "print(\"False Positive Rate: {0}% of bad loans [ROI<-20%] were identified as a good loan [ROI>-20%]\\n\".format(false_pos_rate))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pred_neg  pred_pos\n",
      "neg      1479       708\n",
      "pos      4228      7146\n",
      "\n",
      "Accuracy: 65.22% of loans were correctly identified\n",
      "\n",
      "Specificity: 64.42999999999999% of loans with ROI<-20% were identified as a bad loan loan\n",
      "\n",
      "Recall/Sensitivity: 65.38000000000001% of loans with ROI>-20% were identified as a good loan\n",
      "\n",
      "Precision: 90.53% of loans identified as a good loan (ROI>-20%) were correct\n",
      "\n",
      "False discovery rate: 9.47% of loans identified as good [ROI>-20%] were actually bad loans [ROI<-20%]\n",
      "\n",
      "False Positive Rate: 35.57% of bad loans [ROI<-20%] were identified as a good loan [ROI>-20%]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_tuned = xgb.XGBClassifier(objective = 'binary:logistic',\n",
    "                              early_stopping_rounds = 5,\n",
    "                              scale_pos_weight = (len(y)-y.sum())/y.sum(),\n",
    "                              seed = 0,\n",
    "                              colsample_bytree = 0.9, \n",
    "                              max_depth = 1, \n",
    "                              n_estimators = 500,\n",
    "                              subsample = 0.5,\n",
    "                              n_jobs = -1)\n",
    "\n",
    "xgb_tuned.fit(x_train, y_train)\n",
    "\n",
    "#Confusion Matrix\n",
    "y_pred = xgb_tuned.predict(x_test)\n",
    "print(pd.DataFrame(confusion_matrix(y_test, y_pred),\n",
    "             columns=['pred_neg', 'pred_pos'], index=['neg', 'pos']))\n",
    "\n",
    "# #CM Calculations\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, model.predict(x_test)).ravel()\n",
    "\n",
    "accuracy = round( 100*(tp + tn) / (tp + tn + fn + fp) ,2)\n",
    "print(\"\\nAccuracy: {0}% of loans were correctly identified\\n\".format(accuracy))\n",
    "\n",
    "specificity = round(tn / (tn + fp),4) * 100 #true negative rate\n",
    "print(\"Specificity: {0}% of loans with ROI<-20% were identified as a bad loan loan\\n\".format(specificity))\n",
    "\n",
    "recall = round(tp / (tp + fn), 4) * 100 #true positive rate\n",
    "print(\"Recall/Sensitivity: {0}% of loans with ROI>-20% were identified as a good loan\\n\".format(recall))\n",
    "\n",
    "precision = round(tp/(tp+fp),4) * 100\n",
    "print(\"Precision: {0}% of loans identified as a good loan (ROI>-20%) were correct\\n\".format(precision))\n",
    "\n",
    "false_discovery_rate = round(fp/(fp+tp),4) * 100\n",
    "print(\"False discovery rate: {0}% of loans identified as good [ROI>-20%] were actually bad loans [ROI<-20%]\\n\".format(false_discovery_rate))\n",
    "\n",
    "false_pos_rate = round(fp / (tn + fp),4) * 100\n",
    "print(\"False Positive Rate: {0}% of bad loans [ROI<-20%] were identified as a good loan [ROI>-20%]\\n\".format(false_pos_rate))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Export Results\n",
    "\n",
    "xgboost_output = pd.concat([x_test, y_test], axis=1)\n",
    "xgboost_output['y_predict'] = y_pred\n",
    "\n",
    "xgboost_output = pd.merge(xgboost_output, df_cleaned[\"roi_perc\"], left_on = xgboost_output.index, right_on = df_cleaned[\"roi_perc\"].index)\n",
    "\n",
    "xgboost_output.to_csv(path_or_buf = 'xgboost_output.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
